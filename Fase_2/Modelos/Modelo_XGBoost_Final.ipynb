{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7Radacxmuhti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ac69eeb-c05b-4888-841d-54aec3cbf418"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'tech-challenge'...\n",
            "remote: Enumerating objects: 100, done.\u001b[K\n",
            "remote: Counting objects: 100% (100/100), done.\u001b[K\n",
            "remote: Compressing objects: 100% (98/98), done.\u001b[K\n",
            "remote: Total 100 (delta 32), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (100/100), 1.44 MiB | 9.36 MiB/s, done.\n",
            "Resolving deltas: 100% (32/32), done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3832238692.py:12: UserWarning: Parsing dates in %Y-%m-%d %H:%M:%S format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
            "  df['Data'] = pd.to_datetime(df['Data'], dayfirst=True, errors='coerce')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Data   Último Abertura   Máxima   Mínima  Volume            Vol.    Var%  Var Percent Variacao 1/0  Variacao Margem 0,5% Variacao Margem 0,5% 1/0  mm_5 mm_5 1/0  mm_10 mm_10 1/0  mm_21 mm_21 1/0  Correlacao_Volume_Variação\n",
            "0  30/06/2025  138.855  136.865  139.103  136.430   7,68B   7.680.000.000  0.0145         1.45            1                  0.95                        1  0.34        1   0.12         1   0.01         1                           0\n",
            "1  27/06/2025  136.866  137.113  137.209  136.469   6,24B   6.240.000.000 -0.0018        -0.18            0                  0.00                        0 -0.03        0  -0.06         0  -0.07         0                           1\n",
            "2  26/06/2025  137.114  135.767  137.353  135.756   8,02B   8.020.000.000  0.0099         0.99            1                  0.49                        1 -0.23        0   0.00         1  -0.08         0                           1\n",
            "3  25/06/2025  135.767  137.163  137.163  135.565   7,71B   7.710.000.000 -0.0102        -1.02            0                 -0.52                        0 -0.44        0  -0.05         0  -0.08         0                           1\n",
            "4  24/06/2025  137.165  136.552  138.156  136.254   8,08B   8.080.000.000  0.0045         0.45            1                  0.00                        0 -0.30        0   0.11         1  -0.02         0                           1\n",
            "5  23/06/2025  136.551  137.116  137.130  135.835   7,73B   7.730.000.000 -0.0041        -0.41            0                  0.00                        0 -0.09        0   0.03         1  -0.02         0                           1\n",
            "6  20/06/2025  137.116  138.715  138.719  136.815  11,29B  11.290.000.000 -0.0115        -1.15            0                 -0.65                        0 -0.10        0   0.07         1  -0.02         0                          -1\n",
            "7  18/06/2025  138.717  138.844  139.161  138.443   8,32B   8.320.000.000 -0.0009        -0.09            0                  0.00                        0  0.23        1   0.12         1  -0.05         0                          -1\n",
            "8  17/06/2025  138.840  139.256  139.497  138.293   8,38B   8.380.000.000 -0.0030        -0.30            0                  0.00                        0  0.35        1   0.09         1  -0.03         0                          -1\n",
            "9  16/06/2025  139.256  137.212  139.988  137.212   7,62B   7.620.000.000  0.0149         1.49            1                  0.99                        1  0.52        1   0.18         1   0.00         1                          -1\n",
            "\n",
            "Informações do DataFrame:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2604 entries, 0 to 2603\n",
            "Data columns (total 19 columns):\n",
            " #   Column                      Non-Null Count  Dtype  \n",
            "---  ------                      --------------  -----  \n",
            " 0   Data                        2601 non-null   object \n",
            " 1   Último                      2604 non-null   object \n",
            " 2   Abertura                    2604 non-null   object \n",
            " 3   Máxima                      2604 non-null   object \n",
            " 4   Mínima                      2604 non-null   object \n",
            " 5   Volume                      2601 non-null   object \n",
            " 6   Vol.                        2604 non-null   object \n",
            " 7   Var%                        2601 non-null   float64\n",
            " 8   Var Percent                 2601 non-null   float64\n",
            " 9   Variacao 1/0                2603 non-null   object \n",
            " 10  Variacao Margem 0,5%        2601 non-null   float64\n",
            " 11  Variacao Margem 0,5% 1/0    2603 non-null   object \n",
            " 12  mm_5                        2601 non-null   float64\n",
            " 13  mm_5 1/0                    2603 non-null   object \n",
            " 14  mm_10                       2601 non-null   float64\n",
            " 15  mm_10 1/0                   2603 non-null   object \n",
            " 16  mm_21                       2601 non-null   float64\n",
            " 17  mm_21 1/0                   2603 non-null   object \n",
            " 18  Correlacao_Volume_Variação  2604 non-null   int64  \n",
            "dtypes: float64(6), int64(1), object(12)\n",
            "memory usage: 386.7+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "#Esta primeira célula é para uma melhor verificação dos campos.\n",
        "#Apenas uma apresnetação formatada visualmente para melhor compreensão dos dados.\n",
        "\n",
        "import pandas as pd\n",
        "# Github com os arquivos\n",
        "!git clone https://github.com/LuFaiotto/tech-challenge.git\n",
        "\n",
        "#Carregar arquivo Ibovespa.\n",
        "df = pd.read_excel('/content/tech-challenge/Fase_2/Ibovespa_Normalizado/Ibovespa_02-01-2015_a_30-06-2025.xlsx', dtype=str)\n",
        "\n",
        "#Formatação dos campos:\n",
        "df['Data'] = pd.to_datetime(df['Data'], dayfirst=True, errors='coerce')\n",
        "df['Data'] = df['Data'].dt.strftime('%d/%m/%Y')\n",
        "\n",
        "#Arredondar colunas numéricas para 2 casas decimais\n",
        "colunas_float = [\n",
        "    'Var Percent', 'mm_5', 'mm_10', 'mm_21',\n",
        "    'Variacao Margem 0,5%', 'Correlacao_Volume_Variação'\n",
        "]\n",
        "\n",
        "#Lista das colunas que devem ter ponto de milhar\n",
        "colunas_milhar = ['Último', 'Abertura', 'Máxima', 'Mínima', 'Vol.']\n",
        "\n",
        "for col in colunas_milhar:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "        df[col] = df[col].apply(lambda x: f'{x:,.0f}'.replace(',', '.'))\n",
        "        df['Var%'] = pd.to_numeric(df['Var%'], errors='coerce').round(4)\n",
        "        df['Var Percent'] = pd.to_numeric(df['Var Percent'], errors='coerce').round(2)\n",
        "        df['Variacao Margem 0,5%'] = pd.to_numeric(df['Variacao Margem 0,5%'], errors='coerce').round(2)\n",
        "        df['Correlacao_Volume_Variação'] = pd.to_numeric(df['Correlacao_Volume_Variação'], errors='coerce').fillna(0).astype(int)\n",
        "        df['mm_5'] = pd.to_numeric(df['mm_5'], errors='coerce').round(2)\n",
        "        df['mm_10'] = pd.to_numeric(df['mm_10'], errors='coerce').round(2)\n",
        "        df['mm_21'] = pd.to_numeric(df['mm_21'], errors='coerce').round(2)\n",
        "\n",
        "#Configurar o pandas para mostrar todas as colunas e rolar horizontalmente\n",
        "pd.set_option('display.max_columns', None)        # Mostra todas as colunas\n",
        "pd.set_option('display.width', None)              # Não quebra linha\n",
        "pd.set_option('display.expand_frame_repr', False) # Impede quebra de linha automática\n",
        "\n",
        "#Exibir as 10 primeiras linhas do arquivo.\n",
        "print (df.head(10))\n",
        "\n",
        "#Exibe um resumo das informações do DataFrame, incluindo tipos de dados e valores ausentes.\n",
        "print(\"\\nInformações do DataFrame:\")\n",
        "print(df.info())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparação de Dados Históricos do Ibovespa para Machine Learning\n",
        "\n",
        "#Descrição:\n",
        "#Realizar o pré-processamento e a limpeza de um arquivo de dados do Ibovespa.\n",
        "#O objetivo principal é preparar os dados para um modelo de aprendizagem de máquina.\n",
        "#Carrega os dados, conversão de tipos, indexação, limpeza e remoção de colunas consideradas irrelevantes.\n",
        "#Tratamento de Valores, conversão de campos para númericos e vazios em NaN.\n",
        "#Ordenação para manter a consistência da série temporal.\n",
        "\n",
        "import pandas as pd\n",
        "#Carregar arquivo Ibovespa.\n",
        "df = pd.read_excel('/content/tech-challenge/Fase_2/Ibovespa_Normalizado/Ibovespa_02-01-2015_a_30-06-2025.xlsx', dtype=str)\n",
        "\n",
        "#Pré-processamento e limpeza dos dados\n",
        "\n",
        "#Converte a coluna 'Data' para o tipo datetime.\n",
        "df['Data'] = pd.to_datetime(df['Data'])\n",
        "\n",
        "#Define a coluna 'Data' como o índice do DataFrame para facilitar a análise de séries temporais.\n",
        "df.set_index('Data', inplace=True)\n",
        "\n",
        "#Removendo colunas indesejadas\n",
        "colunas_remover = ['Volume', 'Var%']\n",
        "df = df.drop(columns=[col for col in colunas_remover if col in df.columns])\n",
        "\n",
        "#Converte a coluna 'Var Percent', que pode estar como tipo 'object', para tipo numérico.\n",
        "df['Var Percent'] = pd.to_numeric(df['Var Percent'], errors='coerce')\n",
        "\n",
        "#Lida com dados faltantes (NaN) removendo as linhas que os contêm.\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "#Garante que os dados estejam em ordem cronológica.\n",
        "df.sort_index(inplace=True)\n",
        "\n",
        "#Exibe as informações do DataFrame após a limpeza para verificar os tipos de dados\n",
        "print(\"\\nPrimeiras 15 linhas do DataFrame após a limpeza:\")\n",
        "print(df.head(15))\n",
        "print(\"\\nInformações do DataFrame após a limpeza:\")\n",
        "print(df.info())"
      ],
      "metadata": {
        "id": "NDcVZCW7GPpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Treinamento e Avaliação de um Modelo de Classificação XGBoost.\n",
        "\n",
        "#Descrição:\n",
        "#A análise é dividida nas seguintes etapas:\n",
        "#Divisão dos Dados em treino e teste.\n",
        "#A variável alvo (y) é definida como 'Variacao 1/0', e as variáveis preditoras (X) são as demais colunas do DataFrame.\n",
        "#XGBClassifier é instanciado com parâmetros para classificação binária.\n",
        "#Em seguida, o modelo é treinado usando as variáveis preditoras (X_train) e a variável alvo (y_train) do conjunto de treino.\n",
        "#Avaliação do Modelo, o modelo treinado é usado para prever os valores do conjunto de teste (y_pred). O desempenho é avaliado usando várias métricas.\n",
        "#Acurácia, matriz de confusão, uma visualização em mapa de calor (seaborn) que mostra a contagem de previsões corretas e incorretas para cada classe.\n",
        "#Relatório de classificação, fornece métricas detalhadas como precisão, recall e F1-score para cada classe.\n",
        "\n",
        "\n",
        "#Importando as bibliotecas necessárias para o XGBoost e avaliação do modelo.\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#Define as datas de corte para treino e teste.\n",
        "data_treino_fim = '2025-05-31'\n",
        "data_teste_inicio = '2025-06-01'\n",
        "\n",
        "#Divide o DataFrame usando as datas como índice.\n",
        "df_treino = df.loc[:data_treino_fim]\n",
        "df_teste = df.loc[data_teste_inicio:]\n",
        "\n",
        "#Define a variável alvo (y) e as variáveis preditoras (X) para cada conjunto.\n",
        "#A variável alvo é a coluna 'Variacao 1/0', que indica a tendência.\n",
        "target_column = 'Variacao 1/0'\n",
        "features_columns = df.columns.drop(target_column)\n",
        "\n",
        "#Divide o conjunto de treino em features (X_train) e target (y_train).\n",
        "X_train = df_treino[features_columns]\n",
        "y_train = df_treino[target_column]\n",
        "\n",
        "#Divide o conjunto de teste em features (X_test) e target (y_test).\n",
        "X_test = df_teste[features_columns]\n",
        "y_test = df_teste[target_column]\n",
        "\n",
        "#Exibe o tamanho dos conjuntos para confirmar a divisão.\n",
        "print(\"\\n--- Tamanho dos conjuntos ---\")\n",
        "print(f\"Conjunto de treino (X_train): {X_train.shape}\")\n",
        "print(f\"Conjunto de teste (X_test): {X_test.shape}\")\n",
        "print(f\"Target de treino (y_train): {y_train.shape}\")\n",
        "print(f\"Target de teste (y_test): {y_test.shape}\")\n",
        "\n",
        "#Criação, Treinamento e Avaliação do Modelo XGBoost.\n",
        "#Criação do modelo XGBoost.\n",
        "#O XGBClassifier é usado para problemas de classificação, onde a variável alvo é binária (0 ou 1).\n",
        "model = XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False)\n",
        "\n",
        "#Treinamento do modelo.\n",
        "#O modelo é treinado usando os dados de treino (X_train) e a variável alvo de treino (y_train).\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "#Previsões no conjunto de teste.\n",
        "#O modelo treinado é usado para fazer previsões no conjunto de teste (X_test).\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "#Avaliação do desempenho do modelo.\n",
        "#Calcula a acurácia do modelo comparando as previsões (y_pred) com os valores reais (y_test).\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAcurácia do modelo: {accuracy:.2f}\")\n",
        "\n",
        "#Matriz de Confusão.\n",
        "#Cria e exibe a matriz de confusão para entender o desempenho do modelo em mais detalhes.\n",
        "print(\"\\nMatriz de Confusão:\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Previsto')\n",
        "plt.ylabel('Real')\n",
        "plt.title('Matriz de Confusão')\n",
        "plt.show()\n",
        "\n",
        "#Relatório de Classificação.\n",
        "#Exibe um relatório detalhado com métricas como precisão, recall e F1-score.\n",
        "print(\"\\nRelatório de Classificação:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "jQGUAbcfSZr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Análise de Overfitting em um Modelo de Classificação XGBoost para o Ibovespa\n",
        "\n",
        "#Descrição:\n",
        "#Pré-processar, treinar, analisar um modelo de classificação XGBoost com foco na detecção de overfitting.\n",
        "#O overfitting ocorre quando um modelo se ajusta muito bem aos dados de treino, mas falha em generalizar para novos dados.\n",
        "#Preparação de dados, carrega, limpa os dados históricos do Ibovespa, preparando-os para o modelo.\n",
        "#Divisão em treino e teste, treinamento do Modelo, o XGBClassifier é treinado usando exclusivamente o conjunto de treino.\n",
        "#Análise de Overfitting, é a parte central do script. O modelo treinado é usado para fazer previsões em ambos os conjuntos, de treino e de teste.\n",
        "#As acurácias são calculadas e exibidas lado a lado. A comparação dessas duas métricas é a chave para a análise.\n",
        "#Se a acurácia no treino for significativamente mais alta que a acurácia no teste, é um forte indício de overfitting.\n",
        "#Se as acurácias forem semelhantes e aceitáveis, o modelo provavelmente generalizou bem.\n",
        "#O resultado final é uma exibição clara das acurácias de treino e teste, permitindo uma avaliação direta da capacidade de generalização do modelo.\n",
        "\n",
        "#Importa as bibliotecas necessárias\n",
        "import pandas as pd\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#Carregar arquivo Ibovespa.\n",
        "df = pd.read_excel('/content/tech-challenge/Fase_2/Ibovespa_Normalizado/Ibovespa_02-01-2015_a_30-06-2025.xlsx', dtype=str)\n",
        "\n",
        "df['Data'] = pd.to_datetime(df['Data'])\n",
        "df.set_index('Data', inplace=True)\n",
        "\n",
        "colunas_remover = ['Volume', 'Var%']\n",
        "df = df.drop(columns=[col for col in colunas_remover if col in df.columns])\n",
        "\n",
        "df['Var Percent'] = pd.to_numeric(df['Var Percent'], errors='coerce')\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "df.sort_index(inplace=True)\n",
        "\n",
        "#Divisão dos dados em treino e teste.\n",
        "data_treino_fim = '2025-05-31'\n",
        "data_teste_inicio = '2025-06-01'\n",
        "\n",
        "df_treino = df.loc[:data_treino_fim]\n",
        "df_teste = df.loc[data_teste_inicio:]\n",
        "\n",
        "target_column = 'Variacao 1/0'\n",
        "features_columns = df.columns.drop(target_column)\n",
        "\n",
        "X_train = df_treino[features_columns]\n",
        "y_train = df_treino[target_column]\n",
        "\n",
        "X_test = df_teste[features_columns]\n",
        "y_test = df_teste[target_column]\n",
        "\n",
        "#Criação, Treinamento e Avaliação do Modelo XGBoost.\n",
        "model = XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "#Análise de Overfitting.\n",
        "#Previsões nos conjuntos de treino e teste.\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "#Calcula a acurácia para ambos os conjuntos.\n",
        "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
        "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "#Exibe a comparação das acurácias.\n",
        "print(\"\\n--- Análise de Overfitting ---\")\n",
        "print(f\"Acurácia no conjunto de treino: {accuracy_train:.2f}\")\n",
        "print(f\"Acurácia no conjunto de teste:  {accuracy_test:.2f}\")"
      ],
      "metadata": {
        "id": "1N83PxReWYhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Análise de Estacionariedade de uma Série Temporal do Ibovespa com o Teste de Dickey-Fuller.\n",
        "\n",
        "#Descrição:\n",
        "#Teste de Dickey-Fuller Aumentado (ADF), é um método estatístico padrão para determinar se uma série temporal é estacionária.\n",
        "#A hipótese nula do teste é que a série não é estacionária. Para rejeitar essa hipótese e considerar a série estacionária, o valor de\n",
        "#'p-value' deve ser pequeno (geralmente abaixo de 0.05).\n",
        "#Preparação de Dados, seleção da série temporal, a coluna 'Último' é selecionada para ser a série temporal de interesse, a qual será submetida ao teste.\n",
        "#Execução do Teste ADF, a função adfuller da biblioteca statsmodels é utilizada para realizar o teste. O parâmetro autolag='AIC' permite que o algoritmo\n",
        "#selecione automaticamente o número ideal de defasagens para o teste.\n",
        "#Interpretação dos resultados, os resultados do teste são impressos, incluindo a estatística ADF, o p-valor e os valores críticos. O script então interpreta\n",
        "#o resultado: se o p-valor for menor ou igual a 0.05, a hipótese nula de não-estacionariedade é rejeitada, e a série é considerada estacionária.\n",
        "#Caso contrário, ela é considerada não-estacionária, indicando que transformações nos dados podem ser necessárias antes de usar modelos de previsão.\n",
        "\n",
        "#Importa as bibliotecas necessárias\n",
        "import pandas as pd\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "#Carregar arquivo Ibovespa.\n",
        "df = pd.read_excel('/content/tech-challenge/Fase_2/Ibovespa_Normalizado/Ibovespa_02-01-2015_a_30-06-2025.xlsx', dtype=str)\n",
        "\n",
        "df['Data'] = pd.to_datetime(df['Data'])\n",
        "df.set_index('Data', inplace=True)\n",
        "\n",
        "colunas_remover = ['Volume', 'Var%']\n",
        "df = df.drop(columns=[col for col in colunas_remover if col in df.columns])\n",
        "\n",
        "df['Var Percent'] = pd.to_numeric(df['Var Percent'], errors='coerce')\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "df.sort_index(inplace=True)\n",
        "\n",
        "#Análise de Estacionariedade (Teste ADF)\n",
        "#Seleciona a coluna 'Último' para o teste de estacionariedade\n",
        "serie_temporal = df['Último']\n",
        "\n",
        "#Realiza o Teste de Dickey-Fuller Aumentado\n",
        "resultado_adf = adfuller(serie_temporal, autolag='AIC')\n",
        "\n",
        "#Imprime os resultados do teste\n",
        "print(\"\\n--- Resultados do Teste de Dickey-Fuller Aumentado ---\")\n",
        "print(f\"Estatística ADF: {resultado_adf[0]:.4f}\")\n",
        "print(f\"p-valor: {resultado_adf[1]:.4f}\")\n",
        "print(\"Valores Críticos:\")\n",
        "for chave, valor in resultado_adf[4].items():\n",
        "    print(f\"\\t{chave}: {valor:.4f}\")\n",
        "\n",
        "#Interpretação do resultado\n",
        "if resultado_adf[1] <= 0.05:\n",
        "    print(\"\\nConclusão: A série temporal é estacionária (p-valor <= 0.05).\")\n",
        "else:\n",
        "    print(\"\\nConclusão: A série temporal é não estacionária (p-valor > 0.05).\")"
      ],
      "metadata": {
        "id": "vRnCJkGVYIDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Correção de Não-Estacionariedade por Diferenciação e Validação com o Teste de Dickey-Fuller.\n",
        "\n",
        "#Descrição:\n",
        "#Preparação de dados de séries temporais, a diferenciação para corrigir a não-estacionariedade. A diferenciação é um método comum para transformar uma série não-estacionária\n",
        "#em estacionária, um requisito para que muitos modelos de previsão funcionem corretamente.\n",
        "#Preparação dos Dados, deixando pronta para a análise.\n",
        "#Diferenciação da Série, uma nova coluna, 'Último_Diff', é criada a partir da coluna 'Último'. A diferenciação calcula a diferença entre o valor de um dia e o valor do dia anterior.\n",
        "#Isso remove tendências e sazonalidade, ajudando a estabilizar a média e a variância da série. O valor NaN resultante da primeira linha é removido.\n",
        "#Teste de Estacionariedade, o teste de Dickey-Fuller Aumentado (ADF) é aplicado novamente, mas desta vez na série diferenciada ('Último_Diff').\n",
        "#Conclusão: O script imprime os resultados do teste e interpreta-se. Se o p-valor for menor ou igual a 0.05, a série diferenciada é considerada estacionária.\n",
        "#Isso confirma que a diferenciação foi uma técnica bem-sucedida para corrigir a não-estacionariedade dos dados originais, tornando-os adequados para modelagem de séries temporais.\n",
        "\n",
        "#Bibliotecas necessárias\n",
        "import pandas as pd\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "#Carregar arquivo Ibovespa.\n",
        "df = pd.read_excel('/content/tech-challenge/Fase_2/Ibovespa_Normalizado/Ibovespa_02-01-2015_a_30-06-2025.xlsx', dtype=str)\n",
        "\n",
        "df['Data'] = pd.to_datetime(df['Data'])\n",
        "df.set_index('Data', inplace=True)\n",
        "\n",
        "colunas_remover = ['Volume', 'Var%']\n",
        "df = df.drop(columns=[col for col in colunas_remover if col in df.columns])\n",
        "\n",
        "df['Var Percent'] = pd.to_numeric(df['Var Percent'], errors='coerce')\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "df.sort_index(inplace=True)\n",
        "\n",
        "#Diferenciação e Teste de Estacionariedade.\n",
        "#Cria uma nova série temporal com a diferença da coluna 'Último'.\n",
        "#A diferenciação calcula a diferença entre o valor de um dia e o dia anterior.\n",
        "df['Último_Diff'] = df['Último'].diff().dropna()\n",
        "\n",
        "#O primeiro valor após a diferenciação será NaN, então removemos.\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "#Realiza o Teste de Dickey-Fuller Aumentado na nova série diferenciada.\n",
        "serie_temporal_diferenciada = df['Último_Diff']\n",
        "resultado_adf_diff = adfuller(serie_temporal_diferenciada, autolag='AIC')\n",
        "\n",
        "#Imprime os resultados do teste.\n",
        "print(\"\\n--- Resultados do Teste ADF na Série Diferenciada ---\")\n",
        "print(f\"Estatística ADF: {resultado_adf_diff[0]:.4f}\")\n",
        "print(f\"p-valor: {resultado_adf_diff[1]:.4f}\")\n",
        "print(\"Valores Críticos:\")\n",
        "for chave, valor in resultado_adf_diff[4].items():\n",
        "    print(f\"\\t{chave}: {valor:.4f}\")\n",
        "\n",
        "#Interpretação do resultado.\n",
        "if resultado_adf_diff[1] <= 0.05:\n",
        "    print(\"\\nConclusão: A série temporal diferenciada é estacionária (p-valor <= 0.05).\")\n",
        "else:\n",
        "    print(\"\\nConclusão: A série temporal diferenciada ainda é não estacionária (p-valor > 0.05).\")\n"
      ],
      "metadata": {
        "id": "gBxAt1u7afsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Correção de Não-Estacionariedade por Diferenciação e Validação com o Teste de Dickey-Fuller.\n",
        "\n",
        "#Descrição:\n",
        "#Este é um passo crucial na metodologia de análise de séries temporais. Ele demonstra como corrigir a não-estacionariedade dos dados,\n",
        "#um requisito fundamental para a maioria dos modelos de previsão.\n",
        "#Preparação de dados, a coluna Data é convertida para o tipo datetime e definida como o índice do DataFrame.\n",
        "#Colunas desnecessárias são removidas e linhas com dados faltantes são descartadas.\n",
        "#Diferenciação da série, uma nova coluna, 'Último_Diff', é criada. A diferenciação é uma técnica que calcula a diferença entre o valor de um dia e o valor do dia anterior.\n",
        "#Este processo é eficaz para remover tendências e sazonalidade, transformando uma série não-estacionária em estacionária.\n",
        "#Teste de estacionariedade (ADF), para validar se a diferenciação foi bem-sucedida, o script aplica o Teste de Dickey-Fuller Aumentado (ADF) na nova série diferenciada.\n",
        "#A hipótese nula do teste é que a série não é estacionária.\n",
        "\n",
        "#Conclusão: O código imprime os resultados do teste. Se o p-valor for igual ou menor que 0.05, a hipótese nula é rejeitada, e a série diferenciada é considerada estacionária.\n",
        "#Isso confirma que a transformação foi um sucesso e que os dados agora estão prontos para serem usados em modelos de séries temporais mais robustos.\n",
        "#Ele é um passo crucial na sua metodologia para demonstrar que você corrigiu a não-estacionariedade dos dados, que é um requisito para um modelo de série temporal confiável.\n",
        "\n",
        "# Importa as bibliotecas necessárias.\n",
        "import pandas as pd\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "#Carregar arquivo Ibovespa.\n",
        "df = pd.read_excel('/content/tech-challenge/Fase_2/Ibovespa_Normalizado/Ibovespa_02-01-2015_a_30-06-2025.xlsx', dtype=str)\n",
        "\n",
        "df['Data'] = pd.to_datetime(df['Data'])\n",
        "df.set_index('Data', inplace=True)\n",
        "\n",
        "colunas_remover = ['Volume', 'Var%']\n",
        "df = df.drop(columns=[col for col in colunas_remover if col in df.columns])\n",
        "\n",
        "df['Var Percent'] = pd.to_numeric(df['Var Percent'], errors='coerce')\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "df.sort_index(inplace=True)\n",
        "\n",
        "#Diferenciação e Teste de Estacionariedade.\n",
        "#Cria uma nova série temporal com a diferença da coluna 'Último'.\n",
        "#A diferenciação calcula a diferença entre o valor de um dia e o dia anterior.\n",
        "df['Último_Diff'] = df['Último'].diff()\n",
        "\n",
        "#Remove a primeira linha com NaN gerado pela diferenciação.\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "#Realiza o Teste de Dickey-Fuller Aumentado na nova série diferenciada.\n",
        "serie_temporal_diferenciada = df['Último_Diff']\n",
        "resultado_adf_diff = adfuller(serie_temporal_diferenciada, autolag='AIC')\n",
        "\n",
        "#Imprime os resultados do teste.\n",
        "print(\"\\n--- Resultados do Teste ADF na Série Diferenciada ---\")\n",
        "print(f\"Estatística ADF: {resultado_adf_diff[0]:.4f}\")\n",
        "print(f\"p-valor: {resultado_adf_diff[1]:.4f}\")\n",
        "print(\"Valores Críticos:\")\n",
        "for chave, valor in resultado_adf_diff[4].items():\n",
        "    print(f\"\\t{chave}: {valor:.4f}\")\n",
        "\n",
        "#Interpretação do resultado.\n",
        "if resultado_adf_diff[1] <= 0.05:\n",
        "    print(\"\\nConclusão: A série temporal diferenciada é estacionária (p-valor <= 0.05).\")\n",
        "else:\n",
        "    print(\"\\nConclusão: A série temporal diferenciada ainda é não estacionária (p-valor > 0.05).\")\n"
      ],
      "metadata": {
        "id": "zE65rYqdc4j6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pipeline Completo de Machine Learning: Engenharia de Features e Otimização de Modelo para o Ibovespa.\n",
        "\n",
        "#Descrição:\n",
        "#Representa a culminação das etapas anteriores de preparação de dados, combinando-as com uma engenharia de features avançada e a aplicação de um modelo XGBoost otimizado.\n",
        "#O objetivo é demonstrar uma metodologia completa para a construção de um modelo preditivo robusto, corrigindo problemas identificados previamente, como o overfitting.\n",
        "#Instalação e Importação, garante que a biblioteca xgboost esteja instalada e importa todas as dependências necessárias para a manipulação dos dados,\n",
        "#modelagem e visualização dos resultados.\n",
        "#Preparação de dados base, deixando os dados como um ponto de partida consistente.\n",
        "#Engenharia de features (Lagged Features), esta é a etapa mais importante. Em vez de usar apenas os dados do dia, o script cria features defasadas (lagged features)\n",
        "#para várias colunas-chave (como 'Último', 'Abertura', 'Máxima', etc.), olhando para os 5 dias anteriores. Isso fornece ao modelo o contexto histórico necessário para\n",
        "#tomar decisões, simulando uma situação mais realista e evitando o vazamento de dados.\n",
        "#Divisão dos dados otimizada, os dados são divididos em treino e teste com um período de treino mais curto (de 2023-01-01 a 2025-05-31), focando em dados mais recentes,\n",
        "#o que pode ser mais relevante para a previsão de tendências de mercado.\n",
        "#Treinamento e avaliação do modelo final, um modelo XGBClassifier é treinado usando os melhores parâmetros encontrados em uma otimização anterior.\n",
        "#O desempenho é então avaliado no conjunto de teste, com a exibição da acurácia, da matriz de confusão e do relatório de classificação.\n",
        "#Esses resultados representam a performance final do modelo após a aplicação das melhores práticas de engenharia de features e ajuste de hiperparâmetros, alcançando o objetivo de\n",
        "#acurácia de 75%.\n",
        "\n",
        "#Instala a biblioteca xgboost no ambiente, caso ela não esteja presente.\n",
        "!pip install xgboost\n",
        "\n",
        "#Importa as bibliotecas necessárias.\n",
        "import pandas as pd\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#Carregar arquivo Ibovespa.\n",
        "df = pd.read_excel('/content/tech-challenge/Fase_2/Ibovespa_Normalizado/Ibovespa_02-01-2015_a_30-06-2025.xlsx', dtype=str)\n",
        "\n",
        "df['Data'] = pd.to_datetime(df['Data'])\n",
        "df.set_index('Data', inplace=True)\n",
        "\n",
        "colunas_remover = ['Volume', 'Var%']\n",
        "df = df.drop(columns=[col for col in colunas_remover if col in df.columns])\n",
        "\n",
        "df['Var Percent'] = pd.to_numeric(df['Var Percent'], errors='coerce')\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "df.sort_index(inplace=True)\n",
        "\n",
        "#Engenharia de features avançada (Lagged Features).\n",
        "#Colunas que serão defasadas para criar novas features.\n",
        "features_para_lag = ['Último', 'Abertura', 'Máxima', 'Mínima', 'Vol.',\n",
        "                     'Var Percent', 'Variacao Margem 0,5%',\n",
        "                     'mm_5', 'mm_10', 'mm_21', 'Correlacao_Volume_Variação']\n",
        "\n",
        "#Cria as features defasadas para 1 a 5 dias atrás.\n",
        "for col in features_para_lag:\n",
        "    for i in range(1, 6):  # lag de 1 a 5 dias\n",
        "        df[f'{col}_t-{i}'] = df[col].shift(i)\n",
        "\n",
        "#Remove as primeiras linhas, que conterão valores NaN após a operação de defasagem.\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "#Divisão dos dados em treino e teste (com período de treino de 2 anos).\n",
        "\n",
        "#Redefine as datas de corte conforme a sua sugestão.\n",
        "data_treino_inicio = '2023-01-01'\n",
        "data_treino_fim = '2025-05-31'\n",
        "data_teste_inicio = '2025-06-01'\n",
        "\n",
        "df_treino = df.loc[data_treino_inicio:data_treino_fim]\n",
        "df_teste = df.loc[data_teste_inicio:]\n",
        "\n",
        "target_column = 'Variacao 1/0'\n",
        "features_columns = [col for col in df.columns if '_t-' in col]\n",
        "\n",
        "X_train = df_treino[features_columns]\n",
        "y_train = df_treino[target_column]\n",
        "\n",
        "X_test = df_teste[features_columns]\n",
        "y_test = df_teste[target_column]\n",
        "\n",
        "print(\"\\n--- Tamanho dos conjuntos de dados após engenharia de features e novo período de treino ---\")\n",
        "print(f\"Conjunto de treino (X_train): {X_train.shape}\")\n",
        "print(f\"Conjunto de teste (X_test): {X_test.shape}\")\n",
        "print(f\"Target de treino (y_train): {y_train.shape}\")\n",
        "print(f\"Target de teste (y_test): {y_test.shape}\")\n",
        "\n",
        "#Treinamento e avaliação do modelo.\n",
        "#Usa os melhores parâmetros encontrados na otimização anterior.\n",
        "best_params = {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100}\n",
        "best_model = XGBClassifier(**best_params, use_label_encoder=False, eval_metric='logloss', seed=42)\n",
        "\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_final = best_model.predict(X_test)\n",
        "\n",
        "accuracy_final = accuracy_score(y_test, y_pred_final)\n",
        "print(f\"\\nNova Acurácia Final do modelo: {accuracy_final:.2f}\")\n",
        "\n",
        "print(\"\\nNova Matriz de Confusão Final:\")\n",
        "cm_final = confusion_matrix(y_test, y_pred_final)\n",
        "sns.heatmap(cm_final, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Previsto')\n",
        "plt.ylabel('Real')\n",
        "plt.title('Matriz de Confusão (Resultado Final)')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nNovo Relatório de Classificação Final:\")\n",
        "print(classification_report(y_test, y_pred_final))\n"
      ],
      "metadata": {
        "id": "duaNP8ELqGku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Considerações Finais:**\n",
        "\n",
        "O modelo desenvolvido não apresenta viés tendencioso, alcançando a acurácia exclusivamente a partir dos dados históricos do Ibovespa, devidamente tratados e normalizados.\n",
        "\n",
        "1. Carregamento e Pré-processamento dos Dados\n",
        "Foi realizada a leitura da base de dados, seguida por etapas de limpeza, incluindo:\n",
        "\n",
        "Conversão de tipos inadequados,\n",
        "Tratamento de valores ausentes (missing values),\n",
        "Normalização das colunas relevantes.\n",
        "\n",
        "2. Detecção de Overfitting e Vazamento de Dados\n",
        "O primeiro modelo apresentou acurácia de 100%, o que indicou um vazamento de informações (data leakage). O pipeline foi revisado para garantir o isolamento entre dados de treino e teste.\n",
        "\n",
        "3. Análise de Estacionariedade\n",
        "Identificou-se que a série temporal não era estacionária. Aplicou-se a técnica de diferenciação (differencing) para tornar a série adequada à modelagem preditiva.\n",
        "\n",
        "4. Engenharia de Atributos (Features) e Otimização\n",
        "Nesta etapa-chave:\n",
        "\n",
        "Foram criadas variáveis defasadas (lags) para representar dependências temporais,\n",
        "\n",
        "Ajustou-se o horizonte de previsão,\n",
        "\n",
        "Realizou-se a busca pelos melhores hiperparâmetros via técnicas como Grid Search ou Randomized Search.\n",
        "\n",
        "A combinação dessas estratégias, juntamente com a redução do período de treino para 2,5 anos, para evitar sobreajuste, resultou em um modelo robusto e com acurácia média de 75%, sem sinais de overfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "5asgS-0xjUxT"
      }
    }
  ]
}